{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sapkLOl-j9MH"
      },
      "source": [
        "#Prompting Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1xT--gGj-zg"
      },
      "source": [
        "## Introduction\n",
        "Prompting is the process of designing inputs (or 'prompts') for a language model in a way that guides the model to produce the desired type of response. Effective prompting turns unstructured questions or tasks into structured inputs that the model can process to generate accurate and relevant answers.\n",
        "Prompting in it's core means providing a structured input to an LLM to elicit a specific output or behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFuel47Dkd42"
      },
      "source": [
        "## Real-life Use Cases of Prompting\n",
        "Prompting is crucial in various applications, including but not limited to:\n",
        "- **Content Generation:** Creating articles, reports, and summaries.\n",
        "- **Chatbots:** Enhancing user interactions in customer service, virtual assistance, etc.\n",
        "- **Code Generation:** Assisting developers by generating code snippets and debugging existing code.\n",
        "- **Educational Tools:** Providing explanations, solving math problems, and generating practice questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FkaA0SApVj1"
      },
      "source": [
        "## Example of prompts for gemini model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMhIkAFhj7x6"
      },
      "outputs": [],
      "source": [
        "# Import the required modules\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbqjyrk1lY5S"
      },
      "outputs": [],
      "source": [
        "#Students have to generate their own tokens\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejGL9BeYl9P5",
        "outputId": "f9ffaf96-3d10-4651-e9a2-64e050e40452",
        "colab": {
          "referenced_widgets": [
            "2f95a399e6df41acb128eb389ca30727",
            "ef1c42a69953433480a560641b485343",
            "6d17e24103784304b6d23416deee4c1e",
            "c64e4c8014dd4aec8c8afbd4f0f87d38",
            "96276f8aab094dd1a49c79e1fa447e26",
            "562d80664e254b3a80aaa1c88246473a",
            "371e6ad786b44f449a48e4d829434ca2",
            "9151e18233bb48b2ba53e4707e180e6e",
            "ecd9e47897234772bff1dbf05b534762",
            "95f9a1d828e0430b866c88e7ce1271f7",
            "f65ad2235a4e4308b9f5f727b28b7557"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f95a399e6df41acb128eb389ca30727"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqS6f6Trn9MU",
        "outputId": "7138bd9c-058e-47be-b753-37b46e225e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# device = \"cpu\"\n",
        "model.to(device)\n",
        "print(device)\n",
        "\n",
        "# Function to generate text from the model\n",
        "def generate_text(prompt, max_length=150):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOpJgBe6n-dT",
        "outputId": "bf3d270d-4d59-467a-f4d5-04420f10a1e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Bad Prompt:\n",
            "Tell me about NLP.\n",
            "\n",
            "I’m a big fan of NLP. I’ve been using it for years. I’ve been studying it for years. I’ve been teaching it for years. I’ve been practicing it for years. I’ve been using it in my own life for years. I’ve been using it in my own business for years. I’ve been using it in my own relationships for years. I’ve been using it in my own family for years. I’ve been using it in my own community for years. I’ve been using it in my own country for years. I’ve been using it in my own world for years. I’ve been using it in\n"
          ]
        }
      ],
      "source": [
        "# Bad prompt example: vague and unspecific\n",
        "bad_prompt = \"Tell me about NLP.\"\n",
        "bad_response = generate_text(bad_prompt)\n",
        "print(\"Response to Bad Prompt:\")\n",
        "print(bad_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRms07GFoESf",
        "outputId": "4d4f9a4b-ed74-4ec7-8dbc-644ca6274e99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response to Good Prompt:\n",
            "Explain the significance of Natural Language Processing in enhancing automated customer support systems today.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Step 1/2\n",
            "Natural Language Processing (NLP) is a field of computer science that deals with the interaction between computers and human language. It is used in various applications, including automated customer support systems. In automated customer support systems, NLP is used to understand and respond to customer queries in natural language. This means that the system can understand the customer's question and respond in a way that is natural and easy to understand. NLP is also used to analyze customer feedback and identify patterns and trends. This information can be used to improve the customer experience and provide better support.\n",
            "\n",
            "Step 2/2\n",
            "In addition, NLP can be used\n"
          ]
        }
      ],
      "source": [
        "# Good prompt example: specific and direct\n",
        "good_prompt = \"Explain the significance of Natural Language Processing in enhancing automated customer support systems today.\"\n",
        "good_response = generate_text(good_prompt)\n",
        "print(\"\\nResponse to Good Prompt:\")\n",
        "print(good_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kchE7xcHpG1p"
      },
      "source": [
        "## Impact of Bad Prompts on LLM Performance.\n",
        "\n",
        "The quality of a prompt significantly influences the performance of a language model (LM). Bad prompts can lead to several issues, including poor performance, irrelevant or incorrect information, and potentially biased responses. Understanding these issues helps in crafting better prompts that enhance the utility of LMs in practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "675lbTK8pM1A"
      },
      "source": [
        "###  Vagueness Leading to Poor Performance\n",
        "A vague prompt often results in a generic or off-target response, as the model lacks sufficient direction to generate specific information. This can be particularly problematic in professional or technical settings where accuracy and detail are crucial.\n",
        "\n",
        "*Expected Issue*: The response to this prompt might be overly broad, touching on random aspects of NLP without focusing on current and practical applications, thus not meeting specific user needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX0AAtWZpKUi",
        "outputId": "7cba6700-70ec-4bc5-cb18-fb65d9e35ef9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Vague Prompt:\n",
            "Tell me about NLP.\n",
            "\n",
            "I’m a big fan of NLP. I’ve been using it for years. I’ve been studying it for years. I’ve been teaching it for years. I’ve been practicing it for years. I’ve been using it in my own life for years. I’ve been using it in my own business for years. I’ve been using it in my own relationships for years. I’ve been using it in my own family for years. I’ve been using it in my own community for years. I’ve been using it in my own country for years. I’ve been using it in my own world for years. I’ve been using it in\n"
          ]
        }
      ],
      "source": [
        "# Vague prompt about NLP\n",
        "bad_prompt_vague = \"Tell me about NLP.\"\n",
        "print(\"Response to Vague Prompt:\")\n",
        "print(generate_text(bad_prompt_vague))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UBZeVhzphFE"
      },
      "source": [
        "### Incorrect Scope Leading to Irrelevant Information\n",
        "Prompts that misdirect the focus of the inquiry can lead the LM to generate information that, while correct, is irrelevant to the user's actual needs or intentions.\n",
        "\n",
        "*Expected Issue*: This prompt may lead the model to give a generic answer on how to install NLP into devices while NLP is more of a concept rather than a library to be installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbs1t_yCp0nl",
        "outputId": "6f5bd5de-9744-4557-b71c-388551a434b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Incorrectly Scoped Prompt:\n",
            "How is NLP used in devices?\n",
            "\n",
            "The following are the steps to use NLP in devices:\n",
            "\n",
            "1. <strong>Install the app</strong>\n",
            "\n",
            "  * Download the app from the Google Play Store or Apple App Store.\n",
            "  * Install the app on your device.\n",
            "\n",
            "2. <strong>Create an account</strong>\n",
            "\n",
            "  * Create an account by entering your email address and password.\n",
            "  * You can also sign in with your Google or Facebook account.\n",
            "\n",
            "3. <strong>Connect your device</strong>\n",
            "\n",
            "  * Connect your device to the app by entering the device’s serial number.\n",
            "  * You can also scan the QR code on your device to connect.\n",
            "\n",
            "4. <strong>Start using NLP</strong>\n",
            "\n",
            "  * Once connected\n"
          ]
        }
      ],
      "source": [
        "# Incorrectly scoped prompt\n",
        "bad_prompt_scope = \"How is NLP used in devices?\"\n",
        "print(\"Response to Incorrectly Scoped Prompt:\")\n",
        "print(generate_text(bad_prompt_scope))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJwpeytbp3M9"
      },
      "source": [
        "### Leading Questions Leading to Biased Answers\n",
        "Prompts that are phrased in a way that implies certain answers can lead to biased or skewed responses. This is particularly risky in sensitive areas such as political content, cultural discussions, or ethical issues.\n",
        "\n",
        "Expected Issue: This prompt suggests that NLP is failing, which may cause the model to generate a biased critique of NLP technologies, ignoring their successes and advancements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWICvsJ2qTFG",
        "outputId": "87177a9d-f4be-4bfc-b7a2-0b315adc3734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Biased Prompt:\n",
            "Why is NLP failing in today's tech?\n",
            "\n",
            "The answer is simple: <strong>the NLP models are too complex</strong>.\n",
            "\n",
            "The NLP models are too complex because they are based on <strong>neural networks</strong>. Neural networks are a type of machine learning algorithm that is used to model the behavior of a system. They are composed of a series of interconnected nodes, or neurons, that are connected to each other by weighted connections. The weights of these connections are adjusted based on the input data to produce a desired output.\n",
            "\n",
            "The problem with neural networks is that they are very <strong>complex</strong>. They have a lot of parameters that need to be tuned, and they are very <strong>sensitive</strong> to small changes in the input data.\n"
          ]
        }
      ],
      "source": [
        "# Leading prompt suggesting a biased view\n",
        "bad_prompt_biased = \"Why is NLP failing in today's tech?\"\n",
        "print(\"Response to Biased Prompt:\")\n",
        "print(generate_text(bad_prompt_biased))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6qmNtC4qWHZ"
      },
      "source": [
        "### Overly Complex Prompts Leading to Confusion\n",
        "Prompts that are too complex or contain too much information can confuse the model, resulting in garbled or incoherent responses.\n",
        "\n",
        "*Expected Issue*: The prompt's complexity could overwhelm the model, leading to a response that is fragmented, lacks depth, or is completely irrelevant due to the model's inability to process multiple intricate instructions simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EANeFqwqVdq",
        "outputId": "392be3f0-3fc7-4ccc-ee18-c21575154dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Overly Complex Prompt:\n",
            "Explain how NLP, combined with machine learning and five other AI technologies, is revolutionizing the future of all European tech industries in a comparative analysis to Asian markets.\n",
            "\n",
            "The European tech industry is one of the most innovative and competitive in the world. However, it is also one of the most fragmented, with a wide range of different industries and technologies. In this article, we will explore how NLP, combined with machine learning and five other AI technologies, is revolutionizing the future of all European tech industries.\n",
            "\n",
            "<h2>What is NLP?</h2>\n",
            "\n",
            "Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and human language. It is used to process and analyze text, speech, and other forms of natural language\n"
          ]
        }
      ],
      "source": [
        "# Overly complex prompt\n",
        "bad_prompt_complex = \"Explain how NLP, combined with machine learning and five other AI technologies, is revolutionizing the future of all European tech industries in a comparative analysis to Asian markets.\"\n",
        "print(\"Response to Overly Complex Prompt:\")\n",
        "print(generate_text(bad_prompt_complex))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOTpXHxsvno"
      },
      "source": [
        "## Tips for Crafting Effective Prompts for Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg0pxVLwsyhS"
      },
      "source": [
        "### Simplicity\n",
        "Keep your prompts simple and straightforward to avoid confusion and ensure that the model clearly understands the task.\n",
        "\n",
        "**Explanation:**\n",
        "Simple prompts help the model focus on the task without being distracted by unnecessary details or complex sentence structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41zlVwM7svCW",
        "outputId": "03b24b9d-6b80-4e71-d410-d0076ad8908f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Simple Prompt:\n",
            "Summarize the key benefits of using electric vehicles.\n",
            "\n",
            "A 100-W lightbulb generates $95 \\mathrm{~W}$ of heat, which is dissipated through a glass bulb that has a radius of $3.0 \\mathrm{~cm}$ and is $0.50 \\mathrm{~mm}$ thick. What is the difference in temperature between the inner and outer surfaces of the glass?\n",
            "\n",
            "A 100-turn, 2.0-cm-diameter coil is at rest with its axis vertical. A uniform magnetic field $60^{\\circ}$ away from vertical increases from 0.50 T to 1.50 T in 0.60 s. What is the induced\n"
          ]
        }
      ],
      "source": [
        "simple_prompt = \"Summarize the key benefits of using electric vehicles.\"\n",
        "print(\"Response to Simple Prompt:\")\n",
        "print(generate_text(simple_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88khwGQvs9EC"
      },
      "source": [
        "### 2. Precision in Instruction\n",
        "Use clear action words like \"Write,\" \"Classify,\" \"Summarize,\" \"Translate,\" \"Order,\" etc., to specify the type of response you expect from the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UHPAZE3tD7m",
        "outputId": "0104f8f4-1f6c-4ab3-9d10-4f526346b648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Instruction-Based Prompt:\n",
            "List the steps involved in photosynthesis.\n",
            "\n",
            "A 100-turn, 2.0-cm-diameter coil is at rest with its axis vertical. A uniform magnetic field $60^{\\circ}$ away from vertical increases from 0.50 T to 1.50 T in 0.60 s. What is the induced emf in the coil?\n",
            "\n",
            "A 100-turn coil has a radius of 10.0 cm. The coil is placed in a spatially uniform magnetic field of magnitude 0.500 T so that the face of the coil and the magnetic field are perpendicular. Find the magnitude of the induced emf in the coil if the magnetic field (a) is\n"
          ]
        }
      ],
      "source": [
        "instruction_prompt = \"List the steps involved in photosynthesis.\"\n",
        "print(\"Response to Instruction-Based Prompt:\")\n",
        "print(generate_text(instruction_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYFSYgWtESi"
      },
      "source": [
        "### 3. Specificity\n",
        "Be specific about the task and provide detailed information about what you expect from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO-fF0lwtHK5",
        "outputId": "eb09c037-ecae-4854-aaf7-8b9c99109e8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Specific Prompt:\n",
            "Explain how convolutional neural networks differ from traditional neural networks in image recognition tasks.\n",
            "\n",
            "A 100-W lightbulb is plugged into a standard $120-\\mathrm{V}$ (rms) outlet. Find $(a) I_{\\text {mas }}$ and $(b) I_{\\max }$ when a silicone rubber fuse is used to limit the current to $ 6.5\\ \\mathrm{A}$ . $(c)$ Assume a Menlo Park paper clip fuse is used. What is the power dissipated in the fuse? What is the largest current that the wire in the fuse can carry before melting the fuse?\n",
            "\n",
            "A 100-W lightbulb is plugged into a standard $120-\\mathrm{V\n"
          ]
        }
      ],
      "source": [
        "specific_prompt = \"Explain how convolutional neural networks differ from traditional neural networks in image recognition tasks.\"\n",
        "print(\"Response to Specific Prompt:\")\n",
        "print(generate_text(specific_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlX187aNtHaR"
      },
      "source": [
        "### Avoid Over-Complication\n",
        "While details are important, overly clever or complex prompts can lead to imprecise results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9DGoHP-tKtw",
        "outputId": "e34ae55f-ad01-4b34-98ed-fa61cac76be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Overly Complex Prompt:\n",
            "Discuss how, if at all, quantum computing might revolutionize AI by comparing classical and quantum machine learning algorithms in terms of computational advantages in processing large datasets, specifically in neural network training phases.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Quantum computing might revolutionize AI by processing large datasets in a more efficient way. Classical machine learning algorithms are limited in their ability to process large datasets, while quantum\n",
            "computing can process large datasets more efficiently. This could lead to a more accurate and efficient AI system. Additionally, quantum computing could also lead to a more efficient way of training neural networks.\n",
            "\n",
            "Response to Balanced Prompt:\n",
            "Discuss how quantum computing could impact AI, focusing on machine learning algorithms.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Quantum computing could impact AI in a number of ways. One way is by providing a more efficient way to train AI algorithms. Another way is by providing a more accurate way to predict the future.\n",
            "Finally, quantum computing could also help to improve the accuracy of AI algorithms by providing a more accurate way to measure the accuracy of the predictions.\n"
          ]
        }
      ],
      "source": [
        "# Overly complex vs. balanced prompt example\n",
        "complex_prompt = \"Discuss how, if at all, quantum computing might revolutionize AI by comparing classical and quantum machine learning algorithms in terms of computational advantages in processing large datasets, specifically in neural network training phases.\"\n",
        "balanced_prompt = \"Discuss how quantum computing could impact AI, focusing on machine learning algorithms.\"\n",
        "print(\"Response to Overly Complex Prompt:\")\n",
        "print(generate_text(complex_prompt))\n",
        "print(\"\\nResponse to Balanced Prompt:\")\n",
        "print(generate_text(balanced_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visvbubdtK-b"
      },
      "source": [
        "### 5. Positive Instructions\n",
        "Focus on stating what the model should do rather than what it should not do.\n",
        "\n",
        "Explanation: Positive instructions are more direct and facilitate clearer understanding and execution by the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mKT-OtjtSad",
        "scrolled": true,
        "outputId": "18b83aff-d001-42a0-cf3f-cc5680bbd54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Negative Instruction Prompt:\n",
            "Explain blockchain and don't just list the features of blockchain technology.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Blockchain is a distributed ledger technology that allows for the secure and transparent exchange of digital assets. It is a decentralized system that is not controlled by any single entity or organization. Blockchain technology is used to create a secure and transparent system for the exchange of digital assets.\n",
            "Blockchain technology is used to create a secure and transparent system for the exchange of digital assets. Blockchain technology is used to create a secure and transparent system for the exchange of digital assets. Blockchain technology is used to create a secure and transparent system for the exchange of digital assets. Blockchain technology is used to create a secure and transparent system for the exchange of digital assets. Blockchain technology is used to create\n",
            "\n",
            "Response to Positive Instruction Prompt:\n",
            "Explain the benefits and applications of blockchain technology.\n",
            "\n",
            "The following data were taken from the financial statements of Heston Enterprises Inc. for the current fiscal year. Assuming that long-term investments totaled $\\$ 2,100,000$ throughout the year and that total assets were $\\$ 4,000,000$ at the beginning of the year, determine the ratio of liabilities to stockholders' equity. Round to one decimal place.\n",
            "\n",
            "$\\begin{array}{lrrr} \\text{Property, plant, and equipment (net) . . . . . . . . . . . } &&& \\underline{\\underline{\\$\\hspace{5pt}1,600,000}}\\\\\n"
          ]
        }
      ],
      "source": [
        "# Negative vs. positive instruction example\n",
        "negative_instruction = \"Explain blockchain and don't just list the features of blockchain technology.\"\n",
        "positive_instruction = \"Explain the benefits and applications of blockchain technology.\"\n",
        "print(\"Response to Negative Instruction Prompt:\")\n",
        "print(generate_text(negative_instruction))\n",
        "print(\"\\nResponse to Positive Instruction Prompt:\")\n",
        "print(generate_text(positive_instruction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOhQcodCpMuA"
      },
      "source": [
        "## Homework Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "134ni95GwMY0"
      },
      "source": [
        "Exploring the quality of prompts and how it affects the performance of different Large Language Models (LLMs) across the following domains: common reasoning (e.g., benchmark CoQA dataset), math skills, coding skills, literary skills (e.g., write a romance short in Jane Austen style). Design at least 1  effective and 1 ineffective prompt for each domain, and document the models' responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9iqE3w90phb"
      },
      "source": [
        "## Resources:\n",
        "\n",
        "https://www.deeplearning.ai/short-courses/large-multimodal-model-prompting-with-gemini/\n",
        "\n",
        "This course is for multimodal LLMs, so you will get a broader perspective prompt structuring wrt not just text based but domains.\n",
        "\n",
        "https://www.promptingguide.ai/\n",
        "A very good prompt engineering guide, includes meta prompting, zero shot prompting, few shot prompting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Reasonning"
      ],
      "metadata": {
        "id": "CVeYEmkXgHLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZurMqysYO-G",
        "outputId": "10fb2a01-c305-4833-d4ed-88b333cc027e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to ineffective prompt:\n",
            "What is a roller coaster?\n",
            "\n",
            "A roller coaster is a type of amusement park ride that consists of a train that moves up and down a series of hills and loops. The train is usually suspended from cables or tracks, and the ride is often designed to give riders a feeling of weightlessness or to simulate the feeling of flying. Roller coasters are popular attractions at amusement parks and theme parks, and they have been around for centuries.\n",
            "\n",
            "What is a roller coaster?\n",
            "\n",
            "A roller coaster is a type of amusement park ride that consists of a train that moves up and down a series of hills and loops. The train is usually suspended from cables or tracks, and the ride is often designed to give riders a feeling of weightlessness or to simulate the\n",
            "\n",
            "Response to effective Prompt:\n",
            "'Today it was a roller coaster of the day, I was happy and sad.' What the roller coaster refer to in the context of the previous text?\n",
            "\n",
            "Answer:\n",
            "\n",
            "Step 1/2\n",
            "The roller coaster refers to a type of amusement park ride that involves going up and down a series of hills and curves.\n",
            "\n",
            "Step 2/2\n",
            "In the context of the previous text, it is likely that the roller coaster refers to the emotional roller coaster that the speaker experienced during the day. They may have been happy at one moment and sad at another, or vice versa.\n"
          ]
        }
      ],
      "source": [
        "cr_ineffective = \"What is a roller coaster?\"\n",
        "cr_effective = \"'Today it was a roller coaster of the day, I was happy and sad.' What the roller coaster refer to in the context of the previous text?\"\n",
        "print(\"Response to ineffective prompt:\")\n",
        "print(generate_text(cr_ineffective))\n",
        "print(\"\\nResponse to effective Prompt:\")\n",
        "print(generate_text(cr_effective))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Math Skills"
      ],
      "metadata": {
        "id": "pf1Ea9uYf-01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7R5_n1GYO-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e105b4-1a6f-4954-da0e-1d7aa89c8981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to ineffective prompt:\n",
            "Solve: 3x + 5 = 20\n",
            "\n",
            "Solve: 3x - 5 = 20\n",
            "\n",
            "Solve: 3x + 5 = 20\n",
            "\n",
            "Solve: 3x - 5 = 20\n",
            "\n",
            "Solve: 3x + 5 = 20\n",
            "\n",
            "Solve: 3x - 5 = 20\n",
            "\n",
            "Solve: 3x + 5 = 20\n",
            "\n",
            "Solve: 3x - 5 = 20\n",
            "\n",
            "Solve: 3x + 5 = 20\n",
            "\n",
            "Solve: 3x - 5 = 20\n",
            "\n",
            "Solve: 3x + 5 = 20\n",
            "\n",
            "Solve: 3x -\n",
            "\n",
            "Response to effective Prompt:\n",
            "Return the value of x so that the value stisfies the eqaution 3x + 5 = 20\n",
            "\n",
            "Answer:\n",
            "\n",
            "Step 1/2\n",
            "First, we need to isolate x on one side of the equation. To do this, we can subtract 5 from both sides: 3x + 5 - 5 = 20 - 5 Simplifying: 3x = 15\n",
            "\n",
            "Step 2/2\n",
            "Next, we can solve for x by dividing both sides by 3: 3x/3 = 15/3 Simplifying: x = 5 Therefore, the value of x that satisfies the equation 3x + 5 = 20\n"
          ]
        }
      ],
      "source": [
        "cr_ineffective = \"Solve: 3x + 5 = 20\"\n",
        "cr_effective = \"Return the value of x so that the value stisfies the eqaution 3x + 5 = 20\"\n",
        "print(\"Response to ineffective prompt:\")\n",
        "print(generate_text(cr_ineffective))\n",
        "print(\"\\nResponse to effective Prompt:\")\n",
        "print(generate_text(cr_effective))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Skills"
      ],
      "metadata": {
        "id": "1SKEqNdVf6m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_ineffective = \"Make a code for factorial.\"\n",
        "cr_effective = \"Write a Python function that calculates the factorial of a number. Include error handling for non-integer inputs.\"\n",
        "print(\"Response to ineffective prompt:\")\n",
        "print(generate_text(cr_ineffective))\n",
        "print(\"\\nResponse to effective Prompt:\")\n",
        "print(generate_text(cr_effective))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDErllYAcUtO",
        "outputId": "fa9a4ee4-45c4-46bb-a57c-ab06fd3d8c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to ineffective prompt:\n",
            "Make a code for factorial.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Step 1/3\n",
            "1. First, we need to define the factorial function. We can use the following code: def factorial(n): if n == 0: return 1 else: return n * factorial(n-1)\n",
            "\n",
            "Step 2/3\n",
            "2. Next, we need to define a function to calculate the factorial of a number. We can use the following code: def factorial_of_number(number): return factorial(number)\n",
            "\n",
            "Step 3/3\n",
            "3. Finally, we can use these functions to create a code for calculating the factorial of a number. We can use the following code: number = int(input(\"Enter a\n",
            "\n",
            "Response to effective Prompt:\n",
            "Write a Python function that calculates the factorial of a number. Include error handling for non-integer inputs.\n",
            "\n",
            "Answer:\n",
            "\n",
            "def factorial(n): if n < 0: return 0 elif n == 1: return 1 else: return n * factorial(n-1)\n",
            "def factorial(n): if n < 0: return 0 elif n == 1: return 1 else: return n * factorial(n-1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Literary Skills"
      ],
      "metadata": {
        "id": "BQ93AqyLf0Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_ineffective = \"Write a love story.\"\n",
        "cr_effective = \"Write a romantic short story focusing on long distance relationships.\"\n",
        "print(\"Response to ineffective prompt:\")\n",
        "print(generate_text(cr_ineffective))\n",
        "print(\"\\nResponse to effective Prompt:\")\n",
        "print(generate_text(cr_effective))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9WaBCm9dVU-",
        "outputId": "55eb868a-30aa-4c7c-dcbb-2679b82575e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to ineffective prompt:\n",
            "Write a love story.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story is a story about love.\n",
            "\n",
            "A love story\n",
            "\n",
            "Response to effective Prompt:\n",
            "Write a romantic short story focusing on long distance relationships.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Step 1/2\n",
            "Once upon a time, there was a young couple who had been together for many years. They had a strong bond and were deeply in love. However, their relationship was long-distance, and they had to spend most of their time apart. The couple had a daughter, who was their world. She was their everything, and they loved her more than anything in the world. One day, the husband received a promotion that would take him to a new city. He was excited about the new opportunity, but he knew it would mean leaving his wife and daughter behind. He knew it would be difficult, but he was determined to make it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkiK_jZBd9V5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f95a399e6df41acb128eb389ca30727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef1c42a69953433480a560641b485343",
              "IPY_MODEL_6d17e24103784304b6d23416deee4c1e",
              "IPY_MODEL_c64e4c8014dd4aec8c8afbd4f0f87d38"
            ],
            "layout": "IPY_MODEL_96276f8aab094dd1a49c79e1fa447e26"
          }
        },
        "ef1c42a69953433480a560641b485343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562d80664e254b3a80aaa1c88246473a",
            "placeholder": "​",
            "style": "IPY_MODEL_371e6ad786b44f449a48e4d829434ca2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6d17e24103784304b6d23416deee4c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9151e18233bb48b2ba53e4707e180e6e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecd9e47897234772bff1dbf05b534762",
            "value": 2
          }
        },
        "c64e4c8014dd4aec8c8afbd4f0f87d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95f9a1d828e0430b866c88e7ce1271f7",
            "placeholder": "​",
            "style": "IPY_MODEL_f65ad2235a4e4308b9f5f727b28b7557",
            "value": " 2/2 [00:23&lt;00:00,  9.69s/it]"
          }
        },
        "96276f8aab094dd1a49c79e1fa447e26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562d80664e254b3a80aaa1c88246473a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "371e6ad786b44f449a48e4d829434ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9151e18233bb48b2ba53e4707e180e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecd9e47897234772bff1dbf05b534762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95f9a1d828e0430b866c88e7ce1271f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f65ad2235a4e4308b9f5f727b28b7557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}