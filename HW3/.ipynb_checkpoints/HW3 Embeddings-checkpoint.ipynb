{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFO9ZhJbiLJh",
    "outputId": "557bd64b-181a-4a4e-c893-87434528c783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# prompt: Import text file from a ZIP and mount google drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folder_path = \"/content/drive/MyDrive/Natural Language Processing/HW3/emb_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DuJM-xoxtmd",
    "outputId": "16543676-d29d-4b94-9a1a-5a840d80c71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_100.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/content/drive/MyDrive/Natural Language Processing/HW3/emb_data\"\n",
    "filenames = os.listdir(folder_path)\n",
    "documents = []\n",
    "\n",
    "for file in filenames:\n",
    "  try:\n",
    "    with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "        documents.append(f.read())\n",
    "  except:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6eUxHZgx1rfm"
   },
   "outputs": [],
   "source": [
    "len(documents)\n",
    "doc_str = ' '.join(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5vXjfSfdcBqD"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2ThXhwQESGb"
   },
   "source": [
    "###Bag of Words (BoW)\n",
    "Bag of Words (BoW) is a simple and widely used method in natural language processing to convert text data into numerical representations. The core idea is to represent a text (such as a sentence or a document) as a collection of its words, disregarding grammar and word order but keeping multiplicity. Each unique word in the corpus vocabulary is mapped to a feature index, and the text is represented as a vector where each element indicates the count of a word in the text.\n",
    "\n",
    "### Usage Scenarios:\n",
    "\n",
    "Text Classification: BoW can be used to convert documents into a numerical format that machine learning algorithms can process, such as for spam detection or sentiment analysis.\n",
    "Information Retrieval: Useful for retrieving documents similar to a query by comparing the frequency of words.\n",
    "\n",
    "### Link: [documentation link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkYdC_0_Er7z"
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is an enhancement of the BoW model. It not only considers the occurrence of a word in a document but also how important a word is by considering its frequency across multiple documents. The TF-IDF score is the product of two statistics:\n",
    "\n",
    "-Term Frequency (TF): Measures how frequently a term occurs in a document.\n",
    "\n",
    "-Inverse Document Frequency (IDF): Measures how important a term is, considering the inverse of the document frequency (i.e., the total number of documents divided by the number of documents containing the term).\n",
    "The intuition behind TF-IDF is to reduce the weight of common words and increase the weight of rare, informative words.\n",
    "\n",
    "### Usage Scenarios:\n",
    "\n",
    "-Text Mining: TF-IDF is widely used in mining textual information to identify the most significant words in a collection of documents.\n",
    "\n",
    "-Document Similarity: Helps in measuring the similarity between documents by capturing the importance of words more effectively than simple frequency counts.\n",
    "\n",
    "[documentation link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xysOnuh0cGRR",
    "outputId": "c67607e6-fecf-414d-e90a-e15a040baeab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Embedding:\n",
      " [[35  2  1 ...  1  1  1]]\n",
      "TF-IDF Embedding:\n",
      " [[2.92116614e-03 1.66923780e-04 8.34618898e-05 ... 8.34618898e-05\n",
      "  8.34618898e-05 8.34618898e-05]]\n"
     ]
    }
   ],
   "source": [
    "### Traditional Methods\n",
    "\n",
    "# Bag of Words (BoW)\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform([doc_str])\n",
    "print(\"Bag of Words Embedding:\\n\", X_bow.toarray())\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform([doc_str])\n",
    "print(\"TF-IDF Embedding:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxgm2HsGFW6Q"
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "Word2Vec is a popular word embedding technique that represents words in continuous vector space. Unlike Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), which are based on word frequency counts, Word2Vec captures semantic meanings and relationships between words by training a neural network model. It learns vector representations of words where similar words have similar vectors.\n",
    "\n",
    "Word2Vec uses two main architectures:\n",
    "\n",
    "- Continuous Bag of Words (CBOW): Predicts a target word based on its context (neighboring words).\n",
    "\n",
    "- Skip-gram: Predicts the context (neighboring words) based on a target word.\n",
    "\n",
    "The resulting word vectors can capture semantic similarity and relationships, such as \"king\" being close to \"queen\" and \"Paris\" being close to \"France\" in the vector space.\n",
    "\n",
    "###Usage Scenarios:\n",
    "\n",
    "- Natural Language Processing (NLP): Used in various NLP tasks such as text classification, sentiment analysis, named entity recognition, and machine translation.\n",
    "\n",
    "- Similarity Measurement: To find similar words or phrases in a corpus.\n",
    "Feature Representation: As input features for downstream machine learning models.\n",
    "\n",
    "[documentation link](https://https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2H73bKpbHTWL",
    "outputId": "7e22c2ce-cdfa-4fbd-925e-fa1822c20651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 39.0% 649.0/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "words = doc_str.split()\n",
    "word_vectors = [w2v_model[word] for word in words if word in w2v_model]\n",
    "print(\"Word2Vec Embedding:\\n\", word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfCkRvWyGL01"
   },
   "source": [
    "### GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Unlike Word2Vec, which uses local context window methods, GloVe builds a co-occurrence matrix from the corpus and factorizes it to obtain word vectors. The resulting word vectors capture semantic similarities and relationships by considering the global word-word co-occurrence statistics from a corpus.\n",
    "\n",
    "### Usage Scenarios:\n",
    "\n",
    "- Natural Language Processing (NLP): Used in tasks such as text classification, sentiment analysis, named entity recognition, and machine translation.\n",
    "- Similarity Measurement: To find similar words or phrases in a corpus.\n",
    "- Feature Representation: As input features for downstream machine learning models.\n",
    "\n",
    "[documentation link](https://https://github.com/stanfordnlp/GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WV-LgvMiHbCX"
   },
   "outputs": [],
   "source": [
    "# GloVe\n",
    "glove_model = api.load('glove-wiki-gigaword-100')\n",
    "word_vectors_glove = [glove_model[word] for word in words if word in glove_model]\n",
    "print(\"GloVe Embedding:\\n\", word_vectors_glove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JddlonM1Gsar"
   },
   "source": [
    "### FastText\n",
    "\n",
    "FastText is an extension of Word2Vec introduced by Facebook's AI Research (FAIR) lab. It represents words as bags of character n-grams, which allows it to generate vectors for out-of-vocabulary words by summing the vectors of their character n-grams. This makes FastText particularly effective for morphologically rich languages.\n",
    "\n",
    "### Usage Scenarios:\n",
    "\n",
    "- Natural Language Processing (NLP): Used in various NLP tasks such as text classification, sentiment analysis, and named entity recognition.\n",
    "- Handling Out-of-Vocabulary Words: Generates vectors for words not seen during training.\n",
    "\n",
    "[documentation link](https://https://fasttext.cc/docs/en/supervised-tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kSb5F8h3Hc6b"
   },
   "outputs": [],
   "source": [
    "# FastText\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "word_vectors_fasttext = [fasttext_model[word] for word in words if word in fasttext_model]\n",
    "print(\"FastText Embedding:\\n\", word_vectors_fasttext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYFCqLbKHCVy"
   },
   "source": [
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT is a transformer-based model developed by Google that pre-trains deep bidirectional representations by jointly conditioning on both left and right context in all layers. This enables BERT to understand the context of a word based on its surrounding words. BERT is pre-trained on a large corpus and fine-tuned for specific tasks such as question answering and text classification.\n",
    "\n",
    "### Usage Scenarios:\n",
    "\n",
    "- Natural Language Understanding (NLU): Used in tasks like question answering, text classification, and named entity recognition.\n",
    "- Contextual Word Embeddings: Provides context-sensitive embeddings for words.\n",
    "\n",
    "[hugging face link](https://https://huggingface.co/docs/transformers/en/model_doc/bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXEs8DkXHeX5"
   },
   "outputs": [],
   "source": [
    "# BERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "inputs_bert = tokenizer_bert(example_text, return_tensors='pt')\n",
    "outputs_bert = model_bert(**inputs_bert)\n",
    "print(\"BERT Embedding:\\n\", outputs_bert.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fINdZIKHaRB"
   },
   "source": [
    "### GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "GPT is a transformer-based model developed by OpenAI that uses a unidirectional (left-to-right) context to generate text. GPT is pre-trained on a large corpus of text and can be fine-tuned for various natural language generation tasks such as text completion, summarization, and dialogue generation.\n",
    "\n",
    "### Usage Scenarios:\n",
    "\n",
    "- Natural Language Generation (NLG): Used in tasks like text completion, summarization, and chatbot development.\n",
    "- Contextual Text Generation: Generates coherent and contextually relevant text.\n",
    "\n",
    "[hugging face link](https://huggingface.co/docs/transformers/en/model_doc/openai-gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCeEpoKyHf63"
   },
   "outputs": [],
   "source": [
    "# GPT\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt = GPT2Model.from_pretrained('gpt2')\n",
    "inputs_gpt = tokenizer_gpt(example_text, return_tensors='pt')\n",
    "outputs_gpt = model_gpt(**inputs_gpt)\n",
    "print(\"GPT Embedding:\\n\", outputs_gpt.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77eM5jW4IKIe"
   },
   "source": [
    "### Domain specific embeddings:\n",
    "- Medical Domain\n",
    "  - BioWOrd2Vec: [link text](https://github.com/ncbi-nlp/BioWordVec)\n",
    "  - BioBERT: [link text](https://github.com/dmis-lab/biobert)\n",
    "  - SciBERT: [link text](https://github.com/allenai/scibert)\n",
    "  - ClinicalBERT: [link text](https://github.com/kexinhuang12345/clinicalBERT)\n",
    "- Mathematical DOmain\n",
    "  - MathBERT: [link text](https://arxiv.org/abs/2106.07340)\n",
    "- Legal:\n",
    "  - LegalBERT: [link text](https://huggingface.co/nlpaueb/legal-bert-base-uncased)\n",
    "  - VoyageLaw: [link text](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/)\n",
    "  - FastLaw: [link text](https://github.com/jbesomi/fastlaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeNEWY8JHzU0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
