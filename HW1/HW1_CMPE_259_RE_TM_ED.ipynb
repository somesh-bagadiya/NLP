{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eGajpTvZbl8W",
   "metadata": {
    "id": "eGajpTvZbl8W"
   },
   "source": [
    "# CMPE 259: Homework 1: Regular expressions, text normalization, and edit distance\n",
    "\n",
    "The parts that you need to complete are marked as Exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lkzAo9kSbpaa",
   "metadata": {
    "id": "lkzAo9kSbpaa"
   },
   "source": [
    "## Part 0: Initialization & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d56c92a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d56c92a0",
    "outputId": "7650e6e5-9c40-4f5c-d3fb-2bea5fcc0014"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2jEV5pB3bhkq",
   "metadata": {
    "id": "2jEV5pB3bhkq"
   },
   "source": [
    "## Part 1: Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oLP8HLkcbvcG",
   "metadata": {
    "id": "oLP8HLkcbvcG"
   },
   "source": [
    "### Extracting license plate numbers, IDs, emails and mailing addresses from a document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xLT-yj6keZOF",
   "metadata": {
    "id": "xLT-yj6keZOF"
   },
   "source": [
    "#### Document creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefa7f39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "aefa7f39",
    "outputId": "ea562ac1-fb1d-498a-aee5-2e74a5190ec4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enF7P05qebea",
   "metadata": {
    "id": "enF7P05qebea"
   },
   "source": [
    "Extracting license plate numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e2f32e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65e2f32e",
    "outputId": "bf66c9d7-4754-477c-e87e-4b3262155e94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4XUI302', '3A-278']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The format of license plate number is a digit then 2 or 3 letters (one of which can be a \"-\"), and then 3 digits\n",
    "\n",
    "regex = re.compile(r'(\\d{1}[A-Za-z-]{2,3}\\d{3})')\n",
    "lincense_plate_numbers = regex.findall(sentence)\n",
    "lincense_plate_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nzg5Gxx9dzW2",
   "metadata": {
    "id": "nzg5Gxx9dzW2"
   },
   "source": [
    "### Exercise 1-1: Extract the ID numbers from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98e769f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d98e769f",
    "outputId": "9111fff8-cff9-414f-aed0-66cad428c516"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J987492']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The format of the IDs is one character/letter and then 6 digits\n",
    "regex = re.compile(r'([A-Za-z]\\d{6})')\n",
    "ids = regex.findall(sentence)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y3BZc47FeRzR",
   "metadata": {
    "id": "y3BZc47FeRzR"
   },
   "source": [
    "### Exercise 1-2: Extract the email IDs from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11287af4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11287af4",
    "outputId": "dc97a4c8-c984-411b-eab5-cfbf6014045d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myemail123+spam@google.cg', 'jane.doe@sjsu.edu']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'([\\d\\w+.]*[@]+[\\w\\d]+[.]+[a-z]*)')\n",
    "emails = regex.findall(sentence)\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UfCxo2u2erDf",
   "metadata": {
    "id": "UfCxo2u2erDf"
   },
   "source": [
    "### Exercise 1-3: Extract the mailing address from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62515169",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62515169",
    "outputId": "30fd6518-08e8-4f54-a8bd-8aa91c0add51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123 Main street, San Jose, CA']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starts with 3 digit number, then some letters, a comma, letters, a comma\n",
    "regex = re.compile(r'([0-9 ]{3}[A-Za-z ]*[,]{1}[A-Za-z0-9 ]*[,]{1}[\\w ]*)')\n",
    "mailing_address = regex.findall(sentence)\n",
    "mailing_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uM53UdvPevrA",
   "metadata": {
    "id": "uM53UdvPevrA"
   },
   "source": [
    "### Exercise 1-4: Anonymize the license plate numbers by replacing them with the text \"LP_NUM\"\n",
    "\n",
    "The re.sub function is described here: https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6ae08c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ca6ae08c",
    "outputId": "2318169f-12d5-4eab-8ba4-142e1e4a8f48"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I am 20 years old. My previous license plate number was LP_NUM and my new one is LP_NUM. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now replacing license plate numbers with the string \"LP_NUM\"\n",
    "sentence_modified = sentence\n",
    "for i in lincense_plate_numbers:\n",
    "  sentence_modified = sentence_modified.replace(i, \"LP_NUM\")\n",
    "sentence_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rArCsPyMfAeZ",
   "metadata": {
    "id": "rArCsPyMfAeZ"
   },
   "source": [
    "### Exercise 1-5: Replace the ID numbers with the text \"ID_NUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "glVWmQAOfFTU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "glVWmQAOfFTU",
    "outputId": "35d6d1aa-61d1-4619-caf6-e90949e477a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I am 20 years old. My previous license plate number was LP_NUM and my new one is LP_NUM. My ID is ID_NUM and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in ids:\n",
    "  sentence_modified = sentence_modified.replace(i, \"ID_NUM\")\n",
    "sentence_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2ymEa7sfHnL",
   "metadata": {
    "id": "q2ymEa7sfHnL"
   },
   "source": [
    "## Part 2: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F_Y77JT9fjid",
   "metadata": {
    "id": "F_Y77JT9fjid"
   },
   "source": [
    "Count the number of words in the movie_reviews dataset (dataset uploaded in the beginning of this notebook under \"Part 0: Initialization and Setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3239e17c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3239e17c",
    "outputId": "ffaf296e-4883-48ef-932e-ab54b8b05dda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583820"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print number of words in the movie review dataset\n",
    "len(movie_reviews.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86nbNEYwfgwl",
   "metadata": {
    "id": "86nbNEYwfgwl"
   },
   "source": [
    "Load the standard list of punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0929c795",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0929c795",
    "outputId": "fd3ff1b8-84a6-4efe-cb72-8653520369b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations = string.punctuation\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sg5Sc7X9fbpg",
   "metadata": {
    "id": "sg5Sc7X9fbpg"
   },
   "source": [
    "Remove punctation from movie reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18760b2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18760b2a",
    "outputId": "ba145265-839a-4be6-f3e3-dd5e38da5e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338788"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_wo_puncts = [x for x in movie_reviews.words() if x not in punctuations]\n",
    "len(words_wo_puncts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gvl4C9l8f0M-",
   "metadata": {
    "id": "Gvl4C9l8f0M-"
   },
   "source": [
    "Count the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a18f452",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a18f452",
    "outputId": "5692a228-7eb8-4cce-cefe-011e13404dd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39737"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = set(words_wo_puncts)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D2L7yKz3gL-h",
   "metadata": {
    "id": "D2L7yKz3gL-h"
   },
   "source": [
    "Find the 20 most frequent words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "784d9a68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "784d9a68",
    "outputId": "a19705ef-0e05-4783-ec37-107ce5107c54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>76529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>38106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>35576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>34123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>31937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>25195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>21822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>18513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>16107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>15924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>10792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>9961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>9587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>9517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>8634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>7385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "the     76529\n",
       "a       38106\n",
       "and     35576\n",
       "of      34123\n",
       "to      31937\n",
       "is      25195\n",
       "in      21822\n",
       "s       18513\n",
       "it      16107\n",
       "that    15924\n",
       "as      11378\n",
       "with    10792\n",
       "for      9961\n",
       "his      9587\n",
       "this     9578\n",
       "film     9517\n",
       "i        8889\n",
       "he       8864\n",
       "but      8634\n",
       "on       7385\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 highest freq words\n",
    "pd.Series(words_wo_puncts).value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "id9YqucXf6oW",
   "metadata": {
    "id": "id9YqucXf6oW"
   },
   "source": [
    "Load the standard list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de2e57e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9de2e57e",
    "outputId": "52565804-3427-4fbd-e1b1-fb3c8acdaf9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting english stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cL-RrEeCgA0Z",
   "metadata": {
    "id": "cL-RrEeCgA0Z"
   },
   "source": [
    "Count the number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6645fe0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6645fe0",
    "outputId": "b99a3238-3d2f-41fb-95d5-3ac40e5b1255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xBAO2VeWgDiM",
   "metadata": {
    "id": "xBAO2VeWgDiM"
   },
   "source": [
    "### Exercise 2-1: Remove the stopwords from the dataset (similarly to how we removed punctuation above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d584a6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d584a6f",
    "outputId": "fa230f89-3f73-4c39-ad5c-df03b4aaffed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710578"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_wo_puncts_stopwords = [i for i in words_wo_puncts if i not in eng_stopwords]\n",
    "len(words_wo_puncts_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RuUty50kgS2o",
   "metadata": {
    "id": "RuUty50kgS2o"
   },
   "source": [
    "### Exercise 2-2: Find the number of unique words in the dataset now that the stop words have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a92f5eb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a92f5eb5",
    "outputId": "fc590768-8acd-4930-eeac-bfba1cbe3e77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39586"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words without stopwords\n",
    "unique_words = set(words_wo_puncts_stopwords)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bxDxQWNegcny",
   "metadata": {
    "id": "bxDxQWNegcny"
   },
   "source": [
    "### Exercise 2-3: Find the top 20 highest frequency words now that we have removed the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840dded3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "840dded3",
    "outputId": "55e7c9dd-9c52-418e-e420-e4c373eee63a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>9517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>5771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>3690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>2109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character</th>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>1906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>characters</th>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>1815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>1693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "film          9517\n",
       "one           5852\n",
       "movie         5771\n",
       "like          3690\n",
       "even          2565\n",
       "time          2411\n",
       "good          2411\n",
       "story         2169\n",
       "would         2109\n",
       "much          2049\n",
       "character     2020\n",
       "also          1967\n",
       "get           1949\n",
       "two           1911\n",
       "well          1906\n",
       "characters    1859\n",
       "first         1836\n",
       "--            1815\n",
       "see           1749\n",
       "way           1693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 highest freq words after removing stopwords\n",
    "pd.Series(words_wo_puncts_stopwords).value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Z36G7BcgmDF",
   "metadata": {
    "id": "9Z36G7BcgmDF"
   },
   "source": [
    "Find the words that are used only once in the corpus (and print the first few).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70748ea0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70748ea0",
    "outputId": "6f6dfb76-93de-4c66-c32d-48b5c1fe103d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looooot',\n",
       " 'schnazzy',\n",
       " 'timex',\n",
       " 'indiglo',\n",
       " 'jessalyn',\n",
       " 'gilsig',\n",
       " 'ruber',\n",
       " 'jaleel',\n",
       " 'balki',\n",
       " 'wavers',\n",
       " 'statistics',\n",
       " 'snapshot',\n",
       " 'guesswork',\n",
       " 'maryam',\n",
       " 'daylights',\n",
       " 'terraformed',\n",
       " 'stagnated',\n",
       " 'napolean',\n",
       " 'millimeter',\n",
       " 'enmeshed']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 words that are used only once in corpus using hapaxes() function\n",
    "nltk.FreqDist(words_wo_puncts_stopwords).hapaxes()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "US3mRSQ8bDei",
   "metadata": {
    "id": "US3mRSQ8bDei"
   },
   "source": [
    "### Exercise 2-4: Use the PorterStemmer to stem the words in the dataset.\n",
    "\n",
    "Display the first few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nX3r9FfubKdB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nX3r9FfubKdB",
    "outputId": "d884ed08-be5e-49c6-f625-71274329b148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot :  plot\n",
      "two :  two\n",
      "teen :  teen\n",
      "couples :  coupl\n",
      "go :  go\n",
      "church :  church\n",
      "party :  parti\n",
      "drink :  drink\n",
      "drive :  drive\n",
      "get :  get\n",
      "accident :  accid\n",
      "one :  one\n",
      "guys :  guy\n",
      "dies :  die\n",
      "girlfriend :  girlfriend\n",
      "continues :  continu\n",
      "see :  see\n",
      "life :  life\n",
      "nightmares :  nightmar\n",
      "deal :  deal\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words_wo_puncts_stopwords_stemmed = [ps.stem(i) for i in words_wo_puncts_stopwords]\n",
    "\n",
    "for i in range(20):\n",
    "  print(words_wo_puncts_stopwords[i],\": \",words_wo_puncts_stopwords_stemmed[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QEVGhVGTbUMT",
   "metadata": {
    "id": "QEVGhVGTbUMT"
   },
   "source": [
    "### Exercise 2-5: Use the WordNetLemmatizer to lemmatize the words in the dataset.\n",
    "\n",
    "Display the first few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "WZxMzKv4bMdl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZxMzKv4bMdl",
    "outputId": "141e3e04-1236-4d23-e1c4-cb4ff9ae5180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot :  plot\n",
      "two :  two\n",
      "teen :  teen\n",
      "couples :  couple\n",
      "go :  go\n",
      "church :  church\n",
      "party :  party\n",
      "drink :  drink\n",
      "drive :  drive\n",
      "get :  get\n",
      "accident :  accident\n",
      "one :  one\n",
      "guys :  guy\n",
      "dies :  dy\n",
      "girlfriend :  girlfriend\n",
      "continues :  continues\n",
      "see :  see\n",
      "life :  life\n",
      "nightmares :  nightmare\n",
      "deal :  deal\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_wo_puncts_stopwords_lemma = [lemmatizer.lemmatize(i) for i in words_wo_puncts_stopwords]\n",
    "\n",
    "for i in range(20):\n",
    "  print(words_wo_puncts_stopwords[i],\": \",words_wo_puncts_stopwords_lemma[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uMhXtA3RbMJk",
   "metadata": {
    "id": "uMhXtA3RbMJk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "LuWCQWX3bnsD",
   "metadata": {
    "id": "LuWCQWX3bnsD"
   },
   "source": [
    "### Exercise 2-6:\n",
    "a) How many unique words are there once stemming is applied? (show the that performs the computation and outputs the result)\n",
    "\n",
    "b) How many unique words are there once lemmatization is applied? (show the code that performs the computation and outputs the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "N4WAh6UEbqNq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4WAh6UEbqNq",
    "outputId": "3d4fe4fa-23f8-4ba9-f9f5-301c00098cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after stemming:  26101\n",
      "Unique words after lemmatization:  35172\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words after stemming: \", len(set(words_wo_puncts_stopwords_stemmed)))\n",
    "print(\"Unique words after lemmatization: \", len(set(words_wo_puncts_stopwords_lemma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FQOoke3_bvbr",
   "metadata": {
    "id": "FQOoke3_bvbr"
   },
   "source": [
    "## Part 3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o0HA5ds8HL6-",
   "metadata": {
    "id": "o0HA5ds8HL6-"
   },
   "source": [
    "### Exercise 3-1: Use the Penn Tree Bank tokenizer to tokenize the sentence below\n",
    "\n",
    "Print the tokens that the tokenizer produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "204dbae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "204dbae3",
    "outputId": "a1a2bc31-f8f9-4479-bfef-bc4178c51eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'pay', '$', '100.55', 'to', 'settle', 'your', 'bill.', 'Send', 'confirmation', 'to', 'confirm', '@', 'gmail.com', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = 'Please pay $100.55 to settle your bill.  Send confirmation to confirm@gmail.com.'\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "s_tokens = tokenizer.tokenize(s)\n",
    "print(s_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tu9xug2Gxr84",
   "metadata": {
    "id": "Tu9xug2Gxr84"
   },
   "source": [
    "## Part 4: Levenshtein Distance & Alignment\n",
    "\n",
    "Relevant nltk documentation: https://www.nltk.org/api/nltk.metrics.distance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fBsXnDQ-yPPE",
   "metadata": {
    "id": "fBsXnDQ-yPPE"
   },
   "source": [
    "### Exercise 4-1: Use the nltk functions edit_distance to compute the Levenshtein edit-distance between the strings \"intention\" and \"execution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5aaSK4Ehylz7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aaSK4Ehylz7",
    "outputId": "6b313223-2246-4f6d-9945-6902c2a985f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "w1 = \"intention\"\n",
    "w2 = \"execution\"\n",
    "dist = edit_distance(w1,w2)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NKWLhn1RzBGv",
   "metadata": {
    "id": "NKWLhn1RzBGv"
   },
   "source": [
    "### Exercise 4-2: Use the nltk function edit_distance_align to compute the minimum Levenshtein edit-distance based alignment mapping between the two strings \"intention\" and \"execution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "Zc16veVuzBxM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zc16veVuzBxM",
    "outputId": "31e16724-3cac-4a54-f6a8-aa7405329cb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance_align\n",
    "\n",
    "w1 = \"intention\"\n",
    "w2 = \"execution\"\n",
    "dist_align = edit_distance_align(w1,w2)\n",
    "dist_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6bBuvJZxfqj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6bBuvJZxfqj",
    "outputId": "03bf34b6-40c1-4121-e4e8-8cd37db6769d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function edit_distance_align in module nltk.metrics.distance:\n",
      "\n",
      "edit_distance_align(s1, s2, substitution_cost=1)\n",
      "    Calculate the minimum Levenshtein edit-distance based alignment\n",
      "    mapping between two strings. The alignment finds the mapping\n",
      "    from string s1 to s2 that minimizes the edit distance cost.\n",
      "    For example, mapping \"rain\" to \"shine\" would involve 2\n",
      "    substitutions, 2 matches and an insertion resulting in\n",
      "    the following mapping:\n",
      "    [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (4, 5)]\n",
      "    NB: (0, 0) is the start state without any letters associated\n",
      "    See more: https://web.stanford.edu/class/cs124/lec/med.pdf\n",
      "    \n",
      "    In case of multiple valid minimum-distance alignments, the\n",
      "    backtrace has the following operation precedence:\n",
      "    \n",
      "    1. Substitute s1 and s2 characters\n",
      "    2. Skip s1 character\n",
      "    3. Skip s2 character\n",
      "    \n",
      "    The backtrace is carried out in reverse string order.\n",
      "    \n",
      "    This function does not support transposition.\n",
      "    \n",
      "    :param s1, s2: The strings to be aligned\n",
      "    :type s1: str\n",
      "    :type s2: str\n",
      "    :type substitution_cost: int\n",
      "    :rtype: List[Tuple(int, int)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(edit_distance_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bw7RlxDZxoBn",
   "metadata": {
    "id": "Bw7RlxDZxoBn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
